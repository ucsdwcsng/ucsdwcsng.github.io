---
layout: publication
title: "C-Shenron: A Realistic Radar Simulator for End-to-End Autonomous Driving in CARLA"
short_title: "C-Shenron"
tags: Vehicle
cover: /assets/images/c-shenron/c-shenron-integration.png
disp_cover: "False"
conference: "IEEE Vehicular Technology Conference (VTC) 2025"

conference_site: https://ieeexplore.ieee.org/document/11310463
authors: "Satyam Srivastava, Jerry Li, Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia"
author_list:
    - name: Satyam Srivastava*
      email: srivastavasatyam53@gmail.com
    - name: Jerry Li*
      email: jli793@ucr.edu
    - name: Pushkal Mishra*
      url: https://pushkalm11.github.io/
      email:  pumishra@ucsd.edu
    - name: Kshitiz Bansal
      url: https://kshitizbansal.com/
      email:  ksbansal@ucsd.edu
    - name: Dinesh Bharadia
      url: https://dineshb-ucsd.github.io/
      email: dineshb@ucsd.edu
eqcon: true # true for equal contribution

paper: /files/c-shenron-paper.pdf
github: https://github.com/ucsdwcsng/c-shenron
dataset: http://wcsng-41.nrp-nautilus.io:8000/

miscs:
  - content_type: Dataset
    content_url: http://wcsng-41.nrp-nautilus.io:8000/
  - content_type: Driving Videos
    content_url: https://www.youtube.com/playlist?list=PLMklUDp_gXNE2W83f0UNoK7Vrs9QZROIv
  - content_type: Supplementary Material
    content_url: /files/c-shenron-supplementary-materials.pdf

osd: "We implement a realistic radar sensor model in the CARLA simulator which is based on mmWave surface scattering models from Shenron. We provide code for its implementation and along with a dataset of 850000 frames of radar, camera and semantic LiDAR data (4.5 Tb) collected across different routes in CARLA."

description: # all combinations are possible: (title+text+image, title+image, text+image etc), things will be populated in orders
  - title: Carla Radar vs C-Shenron Radar
    image: /assets/images/c-shenron/carla-vs-c-shenron-radar.png
    text: The following image shows a comparison of the radar sensor output from CARLA and C-Shenron. The camera view is from inside the ego vehicle whereas both radar views are in bird's eye view. Clearly from the image, CARLA radar only provides sparse point clouds whereas C-Shenron provides a dense Range-AoA map.
  - title: High Level Implementation
    text:
      <p>The following diagram illustrates a high level overview of our sensor integration into CARLA and the evaluation framework for End-to-End Driving.</p>

      <a href="/assets/images/c-shenron/c-shenron-flowchart.png"><center><img src="/assets/images/c-shenron/c-shenron-flowchart.png" width="80%" style="float:center" ></center> </a>

      <br>
      <p>The <a href="https://github.com/autonomousvision/carla_garage">Transfuser++ model</a> is the state-of-the-art End-to-End driving model that utilizes Camera and LiDAR sensors for perception and path planning. The model is trained on data from an expert driver provided by CARLA and it predicts the future waypoints/direction and the velocity of the ego vehicle. We substitute the LiDAR input with our integrated C-Shenron radar sensor and re-train multiple models with varying radar views. In our results, we showcase that using radar sensors have improved the driving score and overall situational awareness of the model, indicating the accuracy of our sensor.</p>
  - title: Sensor Views
    image: /assets/images/c-shenron/c-shenron.png
    text: Comparison of views from Camera, Semantic LiDAR, and Shenron Radar in CARLA simulator. Like the above image, the camera view is from inside the ego vehicle whereas both radar views are in bird's eye view.

video_matrix:
  title: Sample Videos Collected Across Different Routes in CARLA
  examples:
    - case: "Example 1"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case1/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case1/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case1/Camera+Radar.gif"
      text: "In this situation, the driving agent is attempting to make a left turn at an intersection. The Camera only model becomes stagnant at the intersection once the vehicle from the opposing lane passes by. Whereas the other two models, due to enhanced spatial awareness, do not stop at the intersection as it can see farther and confirm that no vehicle is coming from the opposite lane."
    - case: "Example 2"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case2/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case2/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case2/Camera+Radar.gif"
      text: "In this scene, the driving agent attempts to switch to the left lane. The Camera only model struggles to make the turn and ends up crashing with a vehicle coming from behind. Whereas in the other two models, both LiDAR and Radar detect a car behind and accordingly increase the speed of vehicle before switching the lane."
    - case: "Example 3"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case3/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case3/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case3/Camera+Radar.gif"
      text: "This is a special test scenario in CARLA where the traffic lights in opposing lanes are turned on to test the situational awareness of the driving agent. Here the vehicle is attempting to make a right turn at the intersection when the lights from crossing lane are on. The Camera only model fails to stop in time and crashes into the incoming car from the crossing lane. However the other two models using LiDAR and Radar manage to avoid the crash by stopping abruptly and proceeding only when it's safe."
  overview: # Modify this
    text: "The advancement of self-driving technology is driven by the need for robust perception and navigation systems. Simulators for autonomous driving facilitate the rapid development and testing of navigation algorithms; however, a key issue for most is their inaccurate modeling of the radar sensor. This is a significant drawback as radars offer robust sensing capabilities in adverse weather conditions and occlusions. CARLA, a widely adopted open-source simulator, provides a simplistic radar model that fails to capture the complex physical and material-dependent behavior of real-world radar, leading to a substantial gap in the realism of its simulated data. To address these limitations, we present C-Shenron, a radar simulation framework integrated into CARLA, which generates realistic radar measurements by fusing LiDAR and camera data. C-Shenron also supports configurable radar parameters, multiple sensor placements, and scalable dataset generation. Our evaluations demonstrate that radar-camera fusion models, trained with C-Shenron's generated data, achieve performance equivalent to traditional LiDAR-camera baselines on key metrics from the CARLA leaderboard."
---
