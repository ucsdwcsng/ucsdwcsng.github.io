---
layout: publication
title : "C-Shenron: A Realistic Radar Simulation Framework for CARLA"
short_title: "C-Shenron"
tags: Vehicle
cover: /assets/images/c-shenron/c-shenron.png
disp_cover: "False"
conference: "In Submission"
authors: "Satyam Srivastava, Jerry Li, Pushkal Mishra, Kshitiz Bansal, Dinesh Bharadia"
author_list:
    - name: Satyam Srivastava
      url: https://satyam-53.github.io/
      email: f20190188@pilani.bits-pilani.ac.in
    - name: Jerry Li
      email: jli793@ucr.edu
    - name: Pushkal Mishra
      email:  pumishra@ucsd.edu
    - name: Kshitiz Bansal
      url: https://kshitizbansal.com/
      email:  ksbansal@ucsd.edu
    - name: Dinesh Bharadia
      url: https://dineshb-ucsd.github.io/
      email: dineshb@ucsd.edu
miscs: 
    - content_type: Paper
      content_url: /files/c-shenron_paper.pdf
    - content_type: Supplementary Material
      content_url: /files/c-shenron-supplementary-materials.pdf
    - content_type: Driving Videos
      content_url: https://www.youtube.com/playlist?list=PLMklUDp_gXNE2W83f0UNoK7Vrs9QZROIv
      
banner: "ðŸ“¢ Code and Dataset will be released after the acceptance."
description: # all combinations are possible: (title+text+image, title+image, text+image etc), things will be populated in orders
    - title: High Level Implementation
      text: <p>The following diagram illustrates a high level overview of our sensor integration into CARLA and the evaluation framework for End-to-End Driving.</p>
      
        <a href="/assets/images/c-shenron/c-shenron-flowchart.png"><center><img src="/assets/images/c-shenron/c-shenron-flowchart.png" width="80%" style="float:center" ></center> </a>

        <br>
        <p>The <a href="https://github.com/autonomousvision/carla_garage">Transfuser++ model</a> is the state-of-the-art End-to-End driving model that utilizes Camera and LiDAR sensors for perception and path planning. The model is trained on data from an expert driver provided by CARLA and it predicts the future waypoints/direction and the velocity of the ego vehicle. We substitute the LiDAR input with our integrated C-Shenron radar sensor and re-train multiple models with varying radar views. In our results, we showcase that using radar sensors have improved the driving score and overall situational awareness of the model, indicating the accuracy of our sensor.</p>
    - title: Sensor Views
      image: /assets/images/c-shenron/c-shenron.png
      text: <center>Comparison of views from Camera, Semantic LiDAR, and Shenron Radar in CARLA simulator.</center>

video_matrix:
  title: Sample Videos Collected Across Different Routes in CARLA
  examples:
    - case: "Example 1"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case1/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case1/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case1/Camera+Radar.gif"
      text: "In this situation, the driving agent is attempting to make a left turn at an intersection. The Camera only model becomes stagnant at the intersection once the vehicle from the opposing lane passes by. Whereas the other two models, due to enhanced spatial awareness, do not stop at the intersection as it can see farther and confirm that no vehicle is coming from the opposite lane."
    - case: "Example 2"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case2/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case2/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case2/Camera+Radar.gif"
      text: "In this scene, the driving agent attempts to switch to the left lane. The Camera only model struggles to make the turn and ends up crashing with a vehicle coming from behind. Whereas in the other two models, both LiDAR and Radar detect a car behind and accordingly increase the speed of vehicle before switching the lane."
    - case: "Example 3"
      videos:
        - description: "Camera Only"
          link: "/assets/gif/c-shenron/case3/Camera.gif"
        - description: "Camera + LiDAR"
          link: "/assets/gif/c-shenron/case3/Camera+LiDAR.gif"
        - description: "Camera + Radar"
          link: "/assets/gif/c-shenron/case3/Camera+Radar.gif"
      text: "This is a special test scenario in CARLA where the traffic lights in opposing lanes are turned on to test the situational awareness of the driving agent. Here the vehicle is attempting to make a right turn at the intersection when the lights from crossing lane are on. The Camera only model fails to stop in time and crashes into the incoming car from the crossing lane. However the other two models using LiDAR and Radar manage to avoid the crash by stopping abruptly and proceeding only when it's safe."
  overview: # Modify this
    text: "The advancement of self-driving technology has become a focal point in outdoor robotics, driven by the need for robust and efficient perception systems. This paper addresses the critical role of sensor integration in autonomous vehicles, particularly emphasizing the underutilization of radar compared to cameras and LiDARs. While extensive research has been conducted on the latter two due to the availability of large-scale datasets, radar technology offers unique advantages such as all-weather sensing and occlusion penetration, which are essential for safe autonomous driving. This study presents a novel integration of a realistic radar sensor model within the CARLA simulator, enabling researchers to develop and test navigation algorithms using radar data. Utilizing this radar sensor and showcasing its capabilities in simulation, we demonstrate improved performance in end-to-end driving scenarios. Our findings aim to rekindle interest in radar-based self-driving research and promote the development of algorithms that leverage radar's strengths."
---
